{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmo de redes neuronales convolucionales"
      ],
      "metadata": {
        "id": "HMQFIBpT6HIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Librerias"
      ],
      "metadata": {
        "id": "yT5Vl_yCNi8j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k-RRMmrNh3F",
        "outputId": "25c86dac-8fcf-45e2-9f4e-e3f45459881c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# librería para manejar las flexiones gramaticales en el idioma inglés.\n",
        "!pip install inflect\n",
        "# Manejo de gráficas\n",
        "!pip install scikit-plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw0q8VGsYFI_",
        "outputId": "69307a48-610e-4a54-d581-2507d28d8aa7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (7.0.0)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect) (2.6.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from inflect) (4.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect) (2.16.3)\n",
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.11.4)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # librería Natural Language Toolkit, usada para trabajar con textos\n",
        "import nltk\n",
        "# Punkt permite separar un texto en frases.\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEDm_dMaYPVu",
        "outputId": "fe671fec-34f8-4b75-c66f-9f9c391249ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga todas las palabras vacias, es decir, aquellas que no aportan nada al significado del texto\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcG8akWwYSNC",
        "outputId": "f627836b-39fa-4cbe-9a1a-4cd518041d0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga de paquete WordNetLemmatizer, este es usado para encontrar el lema de cada palabra\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u3gYt_YYXs3",
        "outputId": "628f686e-a4b4-42bc-a9ca-18c0b7004d77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de librerias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "!{sys.executable} -m pip install pandas-profiling\n",
        "\n",
        "import re, string, unicodedata\n",
        "import contractions\n",
        "import inflect\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWP29H1jYgMx",
        "outputId": "529629c1-6e60-4d76-b2da-74ef1311f652"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas-profiling\n",
            "  Downloading pandas_profiling-3.6.6-py2.py3-none-any.whl (324 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ydata-profiling (from pandas-profiling)\n",
            "  Downloading ydata_profiling-4.7.0-py2.py3-none-any.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.9/357.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<1.12,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (1.11.4)\n",
            "Requirement already satisfied: pandas!=1.4.0,<3,>1.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (2.0.3)\n",
            "Requirement already satisfied: matplotlib<3.9,>=3.2 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (3.7.1)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (2.6.4)\n",
            "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (6.0.1)\n",
            "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (3.1.3)\n",
            "Collecting visions[type_image_path]<0.7.7,>=0.7.5 (from ydata-profiling->pandas-profiling)\n",
            "  Downloading visions-0.7.6-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (1.25.2)\n",
            "Collecting htmlmin==0.1.12 (from ydata-profiling->pandas-profiling)\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting phik<0.13,>=0.11.1 (from ydata-profiling->pandas-profiling)\n",
            "  Downloading phik-0.12.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (686 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.1/686.1 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (2.31.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (4.66.2)\n",
            "Collecting seaborn<0.13,>=0.10.1 (from ydata-profiling->pandas-profiling)\n",
            "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multimethod<2,>=1.4 (from ydata-profiling->pandas-profiling)\n",
            "  Downloading multimethod-1.11.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: statsmodels<1,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (0.14.1)\n",
            "Collecting typeguard<5,>=4.1.2 (from ydata-profiling->pandas-profiling)\n",
            "  Downloading typeguard-4.2.1-py3-none-any.whl (34 kB)\n",
            "Collecting imagehash==4.3.1 (from ydata-profiling->pandas-profiling)\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wordcloud>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (1.9.3)\n",
            "Collecting dacite>=1.8 (from ydata-profiling->pandas-profiling)\n",
            "  Downloading dacite-1.8.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba<1,>=0.56.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling->pandas-profiling) (0.58.1)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (1.6.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1->ydata-profiling->pandas-profiling) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=2.11.1->ydata-profiling->pandas-profiling) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata-profiling->pandas-profiling) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba<1,>=0.56.0->ydata-profiling->pandas-profiling) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling->pandas-profiling) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling->pandas-profiling) (2024.1)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from phik<0.13,>=0.11.1->ydata-profiling->pandas-profiling) (1.3.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ydata-profiling->pandas-profiling) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ydata-profiling->pandas-profiling) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ydata-profiling->pandas-profiling) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-profiling) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-profiling) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-profiling) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata-profiling->pandas-profiling) (2024.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels<1,>=0.13.2->ydata-profiling->pandas-profiling) (0.5.6)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling->pandas-profiling) (23.2.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling->pandas-profiling) (3.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels<1,>=0.13.2->ydata-profiling->pandas-profiling) (1.16.0)\n",
            "Building wheels for collected packages: htmlmin\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27080 sha256=b168af591d1dbb0a88798c333ec116e0baede9e2cf5f454e31222d667df72d5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/91/29/a79cecb328d01739e64017b6fb9a1ab9d8cb1853098ec5966d\n",
            "Successfully built htmlmin\n",
            "Installing collected packages: htmlmin, typeguard, multimethod, dacite, imagehash, visions, seaborn, phik, ydata-profiling, pandas-profiling\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.1\n",
            "    Uninstalling seaborn-0.13.1:\n",
            "      Successfully uninstalled seaborn-0.13.1\n",
            "Successfully installed dacite-1.8.1 htmlmin-0.1.12 imagehash-4.3.1 multimethod-1.11.2 pandas-profiling-3.6.6 phik-0.12.4 seaborn-0.12.2 typeguard-4.2.1 visions-0.7.6 ydata-profiling-4.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entendimiento de datos"
      ],
      "metadata": {
        "id": "3Rija0E9ZC3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero importaremos la base dedatos"
      ],
      "metadata": {
        "id": "0NgiAv7Luucc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos = pd.read_csv(\"tipo2_entrenamiento_estudiantes.csv\", sep=',', encoding = 'utf-8') #Importacion de datos"
      ],
      "metadata": {
        "id": "PfzpNwQJOckF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se visualizan las columnas y se ve una pequeña muestra de los datos"
      ],
      "metadata": {
        "id": "GEtVt8t8u2Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(datos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlyviBjjeHuS",
        "outputId": "512d8c24-f523-44cd-bc19-d7dc04350abb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Class\n",
            "0  Muy buena atención y aclaración de dudas por p...      5\n",
            "1  Buen hotel si están obligados a estar cerca de...      3\n",
            "2  Es un lugar muy lindo para fotografías, visite...      5\n",
            "3  Abusados con la factura de alimentos siempre s...      3\n",
            "4  Tuvimos un par de personas en el grupo que rea...      3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datos.info() #Ver tipo de datos de cada columna con numero de datos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5yShe_cPPKq",
        "outputId": "59a3eadc-0b48-4b71-ea4e-287445efca28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7875 entries, 0 to 7874\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  7875 non-null   object\n",
            " 1   Class   7875 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 123.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que no hay nulos ya que el numero de casillas no vacias es el mismo nyumero de filas."
      ],
      "metadata": {
        "id": "ULYPzTi3vVW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats as st\n",
        "\n",
        "textos = datos.copy()\n",
        "textos['Conteo'] = [len(x) for x in textos['Review']]\n",
        "#textos['Moda'] =\n",
        "textos['Max'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos['Review']]\n",
        "textos['Min'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos['Review']]"
      ],
      "metadata": {
        "id": "rgntJiFhZKY2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(textos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fCk20jMZY7n",
        "outputId": "c762f64c-5e53-4de1-8969-c0b4148665c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Class  Conteo  Max  Min\n",
            "0  Muy buena atención y aclaración de dudas por p...      5     252   20    0\n",
            "1  Buen hotel si están obligados a estar cerca de...      3     297   13    1\n",
            "2  Es un lugar muy lindo para fotografías, visite...      5     104   12    1\n",
            "3  Abusados con la factura de alimentos siempre s...      3     422   14    1\n",
            "4  Tuvimos un par de personas en el grupo que rea...      3     419   14    1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparacion de los datos"
      ],
      "metadata": {
        "id": "VHEVOrDIakre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Codigo para quitar caracteres que no hacen parte del codigo Ascci\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word is not None:\n",
        "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            print(\"Found None value in remove_non_ascii function\")\n",
        "    return new_words\n",
        "\n",
        "#Codigo para transformar  los caracteres en minuscula\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    if words is None:\n",
        "        return []\n",
        "\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word is not None:\n",
        "            new_word = word.lower()\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "#Codigo para eliminar puntuaciones innecesarias\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word is not None:\n",
        "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "            if new_word != '':\n",
        "                new_words.append(new_word)\n",
        "    return new_words\n",
        "#Codigo para tokenizar los numeros para que tengan una representacion textual\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit() and word is not None:\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "#Codigo para eliminar palabras que no son nada relevantes en la evaluacion\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    # Lista de palabras de parada en español\n",
        "    stop_words = set([\"de\", \"la\", \"que\", \"el\", \"en\", \"y\", \"a\", \"los\", \"del\", \"se\", \"las\", \"por\", \"un\", \"para\", \"con\", \"no\", \"una\", \"su\", \"al\", \"lo\", \"como\", \"más\", \"pero\", \"sus\", \"le\", \"ya\", \"o\", \"este\", \"sí\", \"porque\", \"esta\", \"entre\", \"cuando\", \"muy\", \"sin\", \"sobre\", \"también\", \"me\", \"hasta\", \"hay\", \"donde\", \"quien\", \"desde\", \"todo\", \"nos\", \"durante\", \"todos\", \"uno\", \"les\", \"ni\", \"contra\", \"otros\", \"ese\", \"eso\", \"ante\", \"ellos\", \"e\", \"esto\", \"mí\", \"antes\", \"algunos\", \"qué\", \"unos\", \"yo\", \"otro\", \"otras\", \"otra\", \"él\", \"tanto\", \"esa\", \"estos\", \"mucho\", \"quienes\", \"nada\", \"muchos\", \"cual\", \"poco\", \"ella\", \"estar\", \"estas\", \"algunas\", \"algo\", \"nosotros\", \"mi\", \"mis\", \"tú\", \"te\", \"ti\", \"tu\", \"tus\", \"ellas\", \"nosotras\", \"vosotros\", \"vosotras\", \"os\", \"mío\", \"mía\", \"míos\", \"mías\", \"tuyo\", \"tuya\", \"tuyos\", \"tuyas\", \"suyo\", \"suya\", \"suyos\", \"suyas\", \"nuestro\", \"nuestra\", \"nuestros\", \"nuestras\", \"vuestro\", \"vuestra\", \"vuestros\", \"vuestras\", \"esos\", \"esas\", \"estoy\", \"estás\", \"está\", \"estamos\", \"estáis\", \"están\", \"esté\", \"estés\", \"estemos\", \"estéis\", \"estén\", \"estaré\", \"estarás\", \"estará\", \"estaremos\", \"estaréis\", \"estarán\", \"estaría\", \"estarías\", \"estaríamos\", \"estaríais\", \"estarían\", \"estaba\", \"estabas\", \"estábamos\", \"estabais\", \"estaban\", \"estuve\", \"estuviste\", \"estuvo\", \"estuvimos\", \"estuvisteis\", \"estuvieron\", \"estuviera\", \"estuvieras\", \"estuviéramos\", \"estuvierais\", \"estuvieran\", \"estuviese\", \"estuvieses\", \"estuviésemos\", \"estuvieseis\", \"estuviesen\", \"estando\", \"estado\", \"estada\", \"estados\", \"estadas\", \"estad\", \"he\", \"has\", \"ha\", \"hemos\", \"habéis\", \"han\", \"haya\", \"hayas\", \"hayamos\", \"hayáis\", \"hayan\", \"habré\", \"habrás\", \"habrá\", \"habremos\", \"habréis\", \"habrán\", \"habría\", \"habrías\", \"habríamos\", \"habríais\", \"habrían\", \"había\", \"habías\", \"habíamos\", \"habíais\", \"habían\", \"hube\", \"hubiste\", \"hubo\", \"hubimos\", \"hubisteis\", \"hubieron\", \"hubiera\", \"hubieras\", \"hubiéramos\", \"hubierais\", \"hubieran\", \"hubiese\", \"hubieses\", \"hubiésemos\", \"hubieseis\", \"hubiesen\", \"habiendo\", \"habido\", \"habida\", \"habidos\", \"habidas\", \"soy\", \"eres\", \"es\", \"somos\", \"sois\", \"son\", \"sea\", \"seas\", \"seamos\", \"seáis\", \"sean\", \"seré\", \"serás\", \"será\", \"seremos\", \"seréis\", \"serán\", \"sería\", \"serías\", \"seríamos\", \"seríais\", \"serían\", \"era\", \"eras\", \"éramos\", \"erais\", \"eran\", \"fui\", \"fuiste\", \"fue\", \"fuimos\", \"fuisteis\", \"fueron\", \"fuera\", \"fueras\", \"fuéramos\", \"fuerais\", \"fueran\", \"fuese\", \"fueses\", \"fuésemos\", \"fueseis\", \"fuesen\", \"siendo\", \"sido\", \"sed\", \"tengo\", \"tienes\", \"tiene\", \"tenemos\", \"tenéis\", \"tienen\", \"tenga\", \"tengas\", \"tengamos\", \"tengáis\", \"tengan\", \"tendré\", \"tendrás\", \"tendrá\", \"tendremos\", \"tendréis\", \"tendrán\", \"tendría\", \"tendrías\", \"tendríamos\", \"tendríais\", \"tendrían\", \"tenía\", \"tenías\", \"teníamos\", \"teníais\", \"tenían\", \"tuve\", \"tuviste\", \"tuvo\", \"tuvimos\", \"tuvisteis\", \"tuvieron\", \"tuviera\", \"tuvieras\", \"tuviéramos\", \"tuvierais\", \"tuvieran\", \"tuviese\", \"tuvieses\", \"tuviésemos\", \"tuvieseis\", \"tuviesen\", \"teniendo\", \"tenido\", \"tenida\", \"tenidos\", \"tenidas\", \"tened\"])\n",
        "    new_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    return new_words\n",
        "#Codigo para ejecucion de los pasos anteriores\n",
        "def preprocessing(words):\n",
        "  words = remove_non_ascii(words)\n",
        "  words = to_lowercase(words)\n",
        "  words = replace_numbers(words)\n",
        "  words = remove_punctuation(words)\n",
        "  words = remove_stopwords(words)\n",
        "  return words"
      ],
      "metadata": {
        "id": "7snQhOSOZbhy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos['Review'] = datos['Review'].apply(contractions.fix) #Aplica la corrección de las contracciones"
      ],
      "metadata": {
        "id": "x76fh7lRZsLQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizar"
      ],
      "metadata": {
        "id": "OKB6Osp2arv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al tokenizar, se divide cada una de las plabaras que quedaron del paso anterior y se creara una columna nueva con una lista por cada fila de las palabras separadas"
      ],
      "metadata": {
        "id": "fDPfbeNiyVP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos['words'] = datos['Review'].apply(word_tokenize)\n",
        "print(datos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRfF0v1HZzGU",
        "outputId": "61daa22c-2899-4a70-beac-4ba6c33fe2ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Class  \\\n",
            "0  Muy buena atención y aclaración de dudas por p...      5   \n",
            "1  Buen hotel si están obligados a estar cerca de...      3   \n",
            "2  Es un lugar muy lindo para fotografías, visite...      5   \n",
            "3  Abusados con la factura de alimentos siempre s...      3   \n",
            "4  Tuvimos un par de personas en el grupo que rea...      3   \n",
            "\n",
            "                                               words  \n",
            "0  [Muy, buena, atención, y, aclaración, de, duda...  \n",
            "1  [Buen, hotel, si, están, obligados, a, estar, ...  \n",
            "2  [Es, un, lugar, muy, lindo, para, fotografías,...  \n",
            "3  [Abusados, con, la, factura, de, alimentos, si...  \n",
            "4  [Tuvimos, un, par, de, personas, en, el, grupo...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datos['words'].dropna()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVJyl-QWZ-aX",
        "outputId": "4b72a5ac-4442-44d0-ac88-ac3cda2fa24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [Muy, buena, atención, y, aclaración, de, duda...\n",
              "1       [Buen, hotel, si, están, obligados, a, estar, ...\n",
              "2       [Es, un, lugar, muy, lindo, para, fotografías,...\n",
              "3       [Abusados, con, la, factura, de, alimentos, si...\n",
              "4       [Tuvimos, un, par, de, personas, en, el, grupo...\n",
              "                              ...                        \n",
              "7870    [Me, parece, buen, sistema, ,, agiliza, el, tr...\n",
              "7871    [Fue, una, escapada, de, un, día, desde, el, c...\n",
              "7872    [La, Plaza, de, la, Revolución, es, un, lugar,...\n",
              "7873    [Es, la, segunda, ocasión, que, me, quedo, en,...\n",
              "7874    [Llegamos, por, casualidad, a, Los, Mercaderes...\n",
              "Name: words, Length: 7875, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto, podemos aplicar el mejor modelo mirando las palabbras mas escenciales"
      ],
      "metadata": {
        "id": "7cpiG7KxygW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos['words'].info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6m3YercaD1l",
        "outputId": "56c9e085-d10f-4ed2-ea97-c135e04b8b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "RangeIndex: 7875 entries, 0 to 7874\n",
            "Series name: words\n",
            "Non-Null Count  Dtype \n",
            "--------------  ----- \n",
            "7875 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 61.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datos['words1']=datos['words'].apply(preprocessing) #Aplica la eliminación del ruido\n",
        "print(datos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzOxbBfibBjw",
        "outputId": "3ce801ee-db41-465b-a4eb-4e127aa522c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Class  \\\n",
            "0  Muy buena atención y aclaración de dudas por p...      5   \n",
            "1  Buen hotel si están obligados a estar cerca de...      3   \n",
            "2  Es un lugar muy lindo para fotografías, visite...      5   \n",
            "3  Abusados con la factura de alimentos siempre s...      3   \n",
            "4  Tuvimos un par de personas en el grupo que rea...      3   \n",
            "\n",
            "                                               words  \\\n",
            "0  [Muy, buena, atención, y, aclaración, de, duda...   \n",
            "1  [Buen, hotel, si, están, obligados, a, estar, ...   \n",
            "2  [Es, un, lugar, muy, lindo, para, fotografías,...   \n",
            "3  [Abusados, con, la, factura, de, alimentos, si...   \n",
            "4  [Tuvimos, un, par, de, personas, en, el, grupo...   \n",
            "\n",
            "                                              words1  \n",
            "0  [buena, atencion, aclaracion, dudas, parte, se...  \n",
            "1  [buen, hotel, si, estan, obligados, cerca, cen...  \n",
            "2  [lugar, lindo, fotografias, visiten, selina, m...  \n",
            "3  [abusados, factura, alimentos, siempre, echan,...  \n",
            "4  [par, personas, grupo, realmente, queriamos, c...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este es el resultado de la base de datos aplicando todas las funciones, la culumna words1 es la columna que usaremos de ahora en adelante."
      ],
      "metadata": {
        "id": "wA8A043ly088"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizar"
      ],
      "metadata": {
        "id": "OZuv3hkoi-T6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para normalizar, primero se deirva cada palabra, luego la siguiente funcion lematiza cada verbo. Al final se unen ambos resultados para tener los datos normalizados"
      ],
      "metadata": {
        "id": "_JyIBFv6z6FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "    return lemmatized_words\n",
        "\n",
        "\n",
        "def stem_and_lemmatize(words):\n",
        "    stems = stem_words(words)\n",
        "    lemmas = lemmatize_verbs(words)\n",
        "    return stems + lemmas\n",
        "\n",
        "\n",
        "datos['words'] = datos['words'].apply(stem_and_lemmatize)\n",
        "print(datos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UWPQKxyi9Y5",
        "outputId": "d43842e1-1c83-4e11-f75a-0c334717cb58"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Class  \\\n",
            "0  Muy buena atención y aclaración de dudas por p...      5   \n",
            "1  Buen hotel si están obligados a estar cerca de...      3   \n",
            "2  Es un lugar muy lindo para fotografías, visite...      5   \n",
            "3  Abusados con la factura de alimentos siempre s...      3   \n",
            "4  Tuvimos un par de personas en el grupo que rea...      3   \n",
            "\n",
            "                                               words  \\\n",
            "0  [m,  , u,  , y,  ,  ,  , b,  , u,  , e,  , n, ...   \n",
            "1  [b,  , u,  , e,  , n,  ,  ,  , h,  , o,  , t, ...   \n",
            "2  [e,  , s,  ,  ,  , u,  , n,  ,  ,  , l,  , u, ...   \n",
            "3  [a,  , b,  , u,  , s,  , a,  , d,  , o,  ,  , ...   \n",
            "4  [t,  , u,  , v,  , i,  , m,  , o,  ,  ,  , u, ...   \n",
            "\n",
            "                                               word1  \n",
            "0  m u y   b u e n a   a t e n c i ó n   y   a c ...  \n",
            "1  b u e n   h o t e l   s i   e s t á n   o b l ...  \n",
            "2  e s   u n   l u g a r   m u y   l i n d o   p ...  \n",
            "3  a b u s a d o   c o n   l a   f a c t u r a   ...  \n",
            "4  t u v i m o   u n   p a r   d e   p e r s o n ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se convierten los datos seleccionados en dataFrame"
      ],
      "metadata": {
        "id": "jGQTX6IF0P39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos['words'] =datos['words'].apply(lambda x: ' '.join(map(str, x)))\n",
        "print(datos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L09oO21rIZb",
        "outputId": "516ad54f-dd98-42d1-8aea-6a732bf81a43"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Review  Class  \\\n",
            "0     Muy buena atención y aclaración de dudas por p...      5   \n",
            "1     Buen hotel si están obligados a estar cerca de...      3   \n",
            "2     Es un lugar muy lindo para fotografías, visite...      5   \n",
            "3     Abusados con la factura de alimentos siempre s...      3   \n",
            "4     Tuvimos un par de personas en el grupo que rea...      3   \n",
            "...                                                 ...    ...   \n",
            "7870  Me parece buen sistema, agiliza el transporte,...      4   \n",
            "7871  Fue una escapada de un día desde el complejo, ...      4   \n",
            "7872  La Plaza de la Revolución es un lugar emblemát...      3   \n",
            "7873  Es la segunda ocasión que me quedo en los cuar...      1   \n",
            "7874  Llegamos por casualidad a Los Mercaderes, un g...      5   \n",
            "\n",
            "                                                  words  \\\n",
            "0     m u y   b u e n a   a t e n c i ó n   y   a c ...   \n",
            "1     b u e n   h o t e l   s i   e s t á n   o b l ...   \n",
            "2     e s   u n   l u g a r   m u y   l i n d o   p ...   \n",
            "3     a b u s a d o   c o n   l a   f a c t u r a   ...   \n",
            "4     t u v i m o   u n   p a r   d e   p e r s o n ...   \n",
            "...                                                 ...   \n",
            "7870  m e   p a r e c   b u e n   s i s t e m a   , ...   \n",
            "7871  f u e   u n a   e s c a p a d a   d e   u n   ...   \n",
            "7872  l a   p l a z a   d e   l a   r e v o l u c i ...   \n",
            "7873  e s   l a   s e g u n d a   o c a s i ó n   q ...   \n",
            "7874  l l e g a m o   p o r   c a s u a l i d a d   ...   \n",
            "\n",
            "                                                  word1  \n",
            "0     m u y   b u e n a   a t e n c i ó n   y   a c ...  \n",
            "1     b u e n   h o t e l   s i   e s t á n   o b l ...  \n",
            "2     e s   u n   l u g a r   m u y   l i n d o   p ...  \n",
            "3     a b u s a d o   c o n   l a   f a c t u r a   ...  \n",
            "4     t u v i m o   u n   p a r   d e   p e r s o n ...  \n",
            "...                                                 ...  \n",
            "7870  m e   p a r e c   b u e n   s i s t e m a   , ...  \n",
            "7871  f u e   u n a   e s c a p a d a   d e   u n   ...  \n",
            "7872  l a   p l a z a   d e   l a   r e v o l u c i ...  \n",
            "7873  e s   l a   s e g u n d a   o c a s i ó n   q ...  \n",
            "7874  l l e g a m o   p o r   c a s u a l i d a d   ...  \n",
            "\n",
            "[7875 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementacion del modelo de redes neuronales convolucionales (CNN)"
      ],
      "metadata": {
        "id": "6yGeysHLacfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las CNN son usadas principalmente en el procesamiento de datos estructurados en trama, como imágenes y, más recientemente, texto.\n",
        "\n",
        "En el procesamiento de imágenes, las CNN utilizan filtros convolucionales para detectar características locales, como bordes y texturas, y luego combinan estas características para formar representaciones jerárquicas más abstractas de la imagen. Esto les permite aprender patrones complejos y realizar tareas como clasificación de imágenes, detección de objetos y segmentación semántica.\n",
        "\n",
        "En el procesamiento de textos, las CNN se utilizan para capturar patrones locales en cadenas de palabras, esto nos permite la clasificación de sentimientos o la categorización de reseñas.\n",
        "\n"
      ],
      "metadata": {
        "id": "WGUZz7XW2Apa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparacion del modelo"
      ],
      "metadata": {
        "id": "BvNtTEtcxHA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar seleccionamos las columnas de interes para el modelo, en este caso es words la cual fue modificada a lo largo del cuaderno y class"
      ],
      "metadata": {
        "id": "mDn0j4pq07Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Datos de entrada\n",
        "X = datos['words']  # Características\n",
        "y = datos['Class']   # Etiquetas\n",
        "\n"
      ],
      "metadata": {
        "id": "OBro66dRfbQ2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir las etiquetas a one-hot encoding\n",
        "y = tf.keras.utils.to_categorical(y - 1, num_classes=5)  # Restar 1 para que las clases vayan de 0 a 4\n",
        "\n"
      ],
      "metadata": {
        "id": "JuDrUsbdjqBR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "L5LtdFX6jwdo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenización y padding de las palabras\n",
        "max_words = 10000\n",
        "max_sequence_length = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_train_pad = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
        "X_test_pad = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
        "\n"
      ],
      "metadata": {
        "id": "WuOV8JJjjyR4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementacion del modelo"
      ],
      "metadata": {
        "id": "6xIn2AMExKnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo CNN\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_words, 100, input_length=max_sequence_length),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "PL0ubUExj0M_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilar el modelo\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "L9uzvchVj26B"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo\n",
        "model.fit(\n",
        "    X_train_pad,\n",
        "    y_train,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_test_pad, y_test)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vBjuayXj5gB",
        "outputId": "d89386d4-192b-4f85-83f9-3208980bcb25"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "99/99 [==============================] - 7s 74ms/step - loss: 1.2339 - accuracy: 0.4622 - val_loss: 1.3439 - val_accuracy: 0.3854\n",
            "Epoch 2/5\n",
            "99/99 [==============================] - 8s 86ms/step - loss: 1.1765 - accuracy: 0.4925 - val_loss: 1.3381 - val_accuracy: 0.3924\n",
            "Epoch 3/5\n",
            "99/99 [==============================] - 8s 76ms/step - loss: 1.1486 - accuracy: 0.5041 - val_loss: 1.3496 - val_accuracy: 0.4006\n",
            "Epoch 4/5\n",
            "99/99 [==============================] - 8s 79ms/step - loss: 1.0946 - accuracy: 0.5363 - val_loss: 1.3612 - val_accuracy: 0.3994\n",
            "Epoch 5/5\n",
            "99/99 [==============================] - 9s 88ms/step - loss: 1.0613 - accuracy: 0.5529 - val_loss: 1.3740 - val_accuracy: 0.3937\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bb77f571f90>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizacion de resultados"
      ],
      "metadata": {
        "id": "xpHA6tZ_xQVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ2ZRtdtj8cH",
        "outputId": "89a4c6f8-4e12-416a-a774-39debab46469"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50/50 [==============================] - 1s 10ms/step - loss: 1.3740 - accuracy: 0.3937\n",
            "Accuracy: 0.3936507999897003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones"
      ],
      "metadata": {
        "id": "eA81CE78xVjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pérdida en el conjunto de entrenamiento:** La pérdida en el conjunto de entrenamiento disminuye de aproximadamente 1.23 en la primera época a alrededor de 1.06 en la quinta época. Esto sugiere que el modelo está mejorando su capacidad para hacer predicciones precisas sobre los datos de entrenamiento a medida que avanza el entrenamiento.\n",
        "\n",
        "**Precisión en el conjunto de entrenamiento:** La precisión en el conjunto de entrenamiento aumenta de aproximadamente 0.46 en la primera época a alrededor de 0.55 en la quinta época. Esto indica que el modelo está mejorando su capacidad para clasificar correctamente los datos de entrenamiento a lo largo del tiempo.\n",
        "\n",
        "**Pérdida en el conjunto de validación:** La pérdida en el conjunto de validación aumenta ligeramente de alrededor de 1.34 en la primera época a alrededor de 1.37 en la quinta época. Esto sugiere que el modelo podría estar experimentando un cierto grado de sobreajuste a los datos de entrenamiento, ya que la pérdida en el conjunto de validación aumenta a medida que avanza el entrenamiento.\n",
        "\n",
        "**Precisión en el conjunto de validación:** La precisión en entre 39-40% a lo largo de las épocas. Esto indica que el modelo no mejora significativamente su capacidad para clasificar correctamente los datos de validación a lo largo del tiempo, lo que podría ser un indicio de que el modelo no generaliza bien a datos transformados."
      ],
      "metadata": {
        "id": "VoU6hwHF46Km"
      }
    }
  ]
}